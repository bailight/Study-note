## **逼近与插值基础概念**
- 逼近目的：简化复杂函数或处理数据，用新函数描述数据变化
- 插值分类：广义插值包含内插、外推，内插用已知点构建函数
## **常见插值多项式**
- 拉格朗日多项式：公式为$L_{n}(x)=\sum_{i=0}^{n} p_{n i}(x) f_{i}$，有其优缺点
- 牛顿多项式：含等距与不等距节点公式，利用有限差分构建
## **样条函数**
- 定义与特性：在区间内函数和部分导数连续，分不同次数
- 三次样条公式：$S_{3}(x)$在每个子区间有特定表达式
## **数据拟合方法**
- 最小二乘法：最小化残差平方和，公式为$S=\sum_{i=1}^{n} r_{i}^{2}$
- 最小模方法：最小化（加权）残差绝对值和
## **其他逼近方法**
- sigmoid函数逼近：用于神经网络等领域，有特定算法步骤
- 切比雪夫多项式：在[-1,1]区间生成，用于插值和积分


### 详细总结
1. **逼近与插值的基本概念**
    - **逼近的目的**：在面对一组数据或复杂函数时，为简化计算和分析，需要获取一个新的函数，它能描述数据的变化规律，且与原函数相似，借助这个新函数可计算原数据集中缺失的值。
    - **插值的分类**
        - **广义插值**：涵盖内插和外推。内插是利用已知数据点构建函数，外推则是根据已有数据点的趋势对数据范围外的值进行估计。
        - **狭义插值**：通过已知的插值节点（数据点）构建满足一定条件的函数。
2. **常见的插值多项式**
    - **拉格朗日多项式**
        - **公式**：$$L_{n}(x)=\sum_{i=0}^{n} p_{n i}(x) f_{i}$$ $$p_{n i}(x)=\frac{{\left(x - x_{0}\right) \cdots\left(x - x_{i - 1}\right)\left(x - x_{i + 1}\right) \cdots\left(x - x_{n}\right)}}{\left(x_{i} - x_{0}\right) \cdots\left(x_{i} - x_{i - 1}\right)\left(x_{i} - x_{i + 1}\right) \cdots\left(x_{i} - x_{n}\right)}$$。当$n = 1$时，$L_{1}(x)=\frac{x - x_{1}}{x_{0}-x_{1}} f_{0}+\frac{x - x_{0}}{x_{1}-x_{0}} f_{1}$；$n = 2$时，$L_{2}(x)=\frac{(x - x_{1})(x - x_{2})}{(x_{0}-x_{1})(x_{0}-x_{2})} f_{0}+\frac{(x - x_{0})(x - x_{2})}{(x_{1}-x_{0})(x_{1}-x_{2})} f_{1}+\frac{(x - x_{0})(x - x_{1})}{(x_{2}-x_{0})(x_{2}-x_{1})} f_{2}$。
        - **特点**：优点是在函数值变化但自变量不变时使用方便，逼近抛物线等函数时所需点数较少；缺点是新增点时需重新构建多项式，点数增多时复杂度大幅上升，且噪声对整个多项式影响显著。
    - **牛顿多项式**
        - **有限差分**：对于等距节点，设$y = f(x)$，$\Delta x = h$，则$\Delta y=\Delta f(x)=f(x+\Delta x)-f(x)$为函数的一阶有限差分，$\Delta^{n} y=\Delta(\Delta^{n - 1} y)$为高阶有限差分。例如，对于函数$P(x)=x^{3}$，$\Delta x = 1$时，$\Delta P(x)=3x^{2}+3x + 1$，$\Delta^{2} P(x)=6x + 6$，$\Delta^{3} P(x)=6$，$n>3$时，$\Delta^{n} P(x)=0$ 。
        - **公式**：等距节点时，$P_{n}(x)=y_{0}+q \Delta y_{0}+\frac{q(q - 1)}{2!} \Delta^{2} y_{0}+\cdots+\frac{q(q - 1) \cdots(q - n + 1)}{n!} \Delta^{n} y_{0}$，其中$q=\frac{x - x_{0}}{h}$；不等距节点时，$P(x)=y_{0}+[x_{0}, x_{1}](x - x_{0})+[x_{0}, x_{1}, x_{2}](x - x_{0})(x - x_{1})+\cdots+[x_{0}, x_{1}, \cdots, x_{n}](x - x_{0})(x - x_{1}) \cdots(x - x_{n - 1})$ 。
        - **特点**：优点是新增点时无需重建整个多项式，逼近抛物线等函数时所需点数较少；缺点是噪声对整个多项式影响较大。
3. **样条函数**
    - **定义与特性**：样条是一类函数，在给定区间内函数及其若干阶导数连续，在每个子区间上是代数多项式。最高阶导数的阶数为样条的次数，样条次数与区间上最高连续导数阶数的差为样条的亏格。例如，折线是一阶样条，亏格为1。
    - **三次样条公式**：在每个子区间$[x_{i}, x_{i + 1}]$上，$S_{3}(x)=\frac{\left(x_{i + 1}-x\right)^{2}\left(2\left(x - x_{i}\right)+h\right)}{h^{3}} f_{i}+\frac{\left(x - x_{i}\right)^{2}\left(2\left(x_{i + 1}-x\right)+h\right)}{h^{3}} f_{i + 1}+\frac{\left(x_{i + 1}-x\right)^{2}\left(x - x_{i}\right)}{h^{2}} m_{i}+\frac{\left(x - x_{i}\right)^{2}\left(x_{i + 1}-x\right)}{h^{2}} m_{i + 1}$ 。
    - **特点**：处理大量数据点时更简便，对输入数据中的噪声处理能力更好；但与多项式插值相比，逼近简单函数（如抛物线）可能较困难，且在处理插值区间边界的子段时，需选择合适的方法。
4. **数据拟合方法**
    - **最小二乘法**：主要用于数据拟合，通过最小化残差平方和来实现。残差$r_{i}=y_{i}-f(x_{i}, \beta)$，其中$(x_{i}, y_{i})$是数据点，$f(x, \beta)$是模型函数，$\beta$是模型参数。最小二乘法通过求解$S=\sum_{i = 1}^{n} r_{i}^{2}$的最小值来确定参数$\beta$ 。
    - **最小模方法**：与最小二乘法类似，区别在于它最小化的是（加权）残差绝对值之和。
5. **其他逼近方法**
    - **sigmoid函数逼近**：公式为$f(\vec{x}) \approx \sum_{k = 1}^{n} \alpha_{k} \sigma(\left<\vec{w}_{k}, \vec{x}\right>+\theta_{k})$ 。算法步骤包括选择sigmoid函数（如逻辑函数$\sigma(t)=\frac{1}{1 + e^{-t}}$）、确定仿射变换、构建逼近函数并最小化系数、使用梯度下降等方法进行优化。该方法应用于神经网络、函数逼近、机器学习中的模式识别等领域。
    - **切比雪夫多项式**：在区间[-1,1]上，$T_{n}(x)=\cos(n \arccos x)$ 。$n = 0$时，$T_{0}(x)=1$；$n = 1$时，$T_{1}(x)=x$ 。通过递推公式$T_{n + 1}(x)=2xT_{n}(x)-T_{n - 1}(x)$（$n = 1,2,\cdots$）可生成更高阶的切比雪夫多项式，如$T_{2}(x)=2x^{2}-1$，$T_{3}(x)=4x^{3}-3x$等，主要用于插值和积分。

|方法|公式|优点|缺点|
|---|---|---|---|
|拉格朗日多项式|$L_{n}(x)=\sum_{i=0}^{n} p_{n i}(x) f_{i}$，$p_{n i}(x)=\frac{{\left(x - x_{0}\right) \cdots\left(x - x_{i - 1}\right)\left(x - x_{i + 1}\right) \cdots\left(x - x_{n}\right)}}{\left(x_{i} - x_{0}\right) \cdots\left(x_{i} - x_{i - 1}\right)\left(x_{i} - x_{i + 1}\right) \cdots\left(x_{i} - x_{n}\right)}$|函数值变化时易用，逼近抛物线点数需求少|新增点需重建，复杂度随点数升，受噪声影响大|
|牛顿多项式（等距节点）|$P_{n}(x)=y_{0}+q \Delta y_{0}+\frac{q(q - 1)}{2!} \Delta^{2} y_{0}+\cdots+\frac{q(q - 1) \cdots(q - n + 1)}{n!} \Delta^{n} y_{0}$，$q=\frac{x - x_{0}}{h}$|新增点方便，逼近抛物线点数需求少|受噪声影响大|
|牛顿多项式（不等距节点）|$P(x)=y_{0}+[x_{0}, x_{1}](x - x_{0})+[x_{0}, x_{1}, x_{2}](x - x_{0})(x - x_{1})+\cdots+[x_{0}, x_{1}, \cdots, x_{n}](x - x_{0})(x - x_{1}) \cdots(x - x_{n - 1})$|新增点方便，逼近抛物线点数需求少|受噪声影响大|
|三次样条|$S_{3}(x)=\frac{\left(x_{i + 1}-x\right)^{2}\left(2\left(x - x_{i}\right)+h\right)}{h^{3}} f_{i}+\frac{\left(x - x_{i}\right)^{2}\left(2\left(x_{i + 1}-x\right)+h\right)}{h^{3}} f_{i + 1}+\frac{\left(x_{i + 1}-x\right)^{2}\left(x - x_{i}\right)}{h^{2}} m_{i}+\frac{\left(x - x_{i}\right)^{2}\left(x_{i + 1}-x\right)}{h^{2}} m_{i + 1}$|处理大量点和噪声好|逼近简单函数可能难，边界需选合适方法|
|最小二乘法|$S=\sum_{i = 1}^{n} r_{i}^{2}$，$r_{i}=y_{i}-f(x_{i}, \beta)$|通过最小化残差平方和拟合数据| - |
|最小模方法|最小化（加权）残差绝对值和| - | - |
|sigmoid函数逼近|$f(\vec{x}) \approx \sum_{k = 1}^{n} \alpha_{k} \sigma(\left<\vec{w}_{k}, \vec{x}\right>+\theta_{k})$|用于神经网络等多领域| - |
|切比雪夫多项式|$T_{n}(x)=\cos(n \arccos x)$，$T_{n + 1}(x)=2xT_{n}(x)-T_{n - 1}(x)$|用于插值和积分| - |
---
### 关键问题
1. **拉格朗日多项式和牛顿多项式在应用上有何不同？**
    - 拉格朗日多项式在函数值变化但自变量不变时使用方便，然而新增点时需重新构建整个多项式，这在实际应用中，若数据频繁更新，计算成本会很高。牛顿多项式在新增点时无需重建整个多项式，更适合数据不断补充的场景。但二者都受噪声影响较大，在噪声较多的数据环境中使用时，可能会导致结果不准确。
2. **三次样条函数为什么在处理大量数据点和噪声数据上表现更好？**
    - 三次样条函数在每个子区间上都是三次多项式，并且函数及其一阶、二阶导数在整个区间内连续，这种特性使得它能够更好地拟合复杂的数据曲线。在处理大量数据点时，它可以通过调整每个子区间上的多项式系数，更灵活地适应数据的变化趋势。同时，由于其连续的导数性质，它对噪声数据有一定的平滑作用，相比其他方法能更有效地减少噪声对整体拟合效果的影响。
3. **最小二乘法和最小模方法的本质区别是什么，这种区别会导致在实际应用中产生怎样不同的结果？**
    - 最小二乘法是通过最小化残差的平方和来确定模型参数，而最小模方法是最小化（加权）残差的绝对值之和。最小二乘法对较大的残差给予更大的权重，因为残差平方会放大较大残差的影响，这使得它在数据误差分布较为均匀时效果较好。最小模方法对所有残差一视同仁，更注重数据的整体偏差情况，在存在异常值（较大误差数据点）时，能减少异常值对结果的影响，使拟合结果更稳健，但可能在拟合精度上不如最小二乘法在理想数据情况下高。 